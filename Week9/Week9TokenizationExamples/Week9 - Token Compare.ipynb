{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_list = ['https://t.co/9z2J3P33Uc',\n",
    "               'laugh/cry',\n",
    "               'ðŸ˜¬ðŸ˜­ðŸ˜“ðŸ¤¢ðŸ™„ðŸ˜±',\n",
    "               \"world's problems\",\n",
    "               \"@datageneral\",\n",
    "                \"It's interesting\",\n",
    "               \"don't spell my name right\",\n",
    "               'all-nighter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD_TOKENIZE separate words using spaces and punctuations.\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = []\n",
    "for sent in compare_list:\n",
    "    print(word_tokenize(sent))\n",
    "    word_tokens.append(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORDPUNCTTOKENIZER splits all punctuations into separate tokens. \n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "punct_tokenizer = WordPunctTokenizer()\n",
    "punct_tokens = []\n",
    "for sent in compare_list:\n",
    "    print(punct_tokenizer.tokenize(sent))\n",
    "    punct_tokens.append(punct_tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REGEXPTOKENIZER control how to tokenize text. \\w+ matches one or more word character (alphanumeric & underscore)\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "match_tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "match_tokens = []\n",
    "for sent in compare_list:   \n",
    "    print(match_tokenizer.tokenize(sent))\n",
    "    match_tokens.append(match_tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''RegexpTokenizer can also work by matching the gaps. When the parameter gaps=True is added, \n",
    "the matching pattern will be used as the separators. \\s+ matches one or more space.'''\n",
    "\n",
    "space_tokenizer = RegexpTokenizer(\"\\s+\", gaps=True)\n",
    "space_tokens = []\n",
    "for sent in compare_list:\n",
    "    print(space_tokenizer.tokenize(sent))\n",
    "    space_tokens.append(space_tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TWEETTOKENIZER the best way to tokenize tweets is to use the tokenizer built to tokenize tweets\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = []\n",
    "for sent in compare_list:\n",
    "    print(tweet_tokenizer.tokenize(sent))\n",
    "    tweet_tokens.append(tweet_tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Put Everything Together\n",
    "Instead of taking the time to analyze the outcome of each tokenizer,\n",
    "we can put everything in one pd.dataframe for fast and accurate interpretation.'''\n",
    "import pandas as pd\n",
    "tokenizers = {'word_tokenize': word_tokens,\n",
    "             'WordPunctTokenize':punct_tokens,\n",
    "             'RegrexTokenizer for matching':match_tokens,\n",
    "             'RegrexTokenizer for white space': space_tokens,\n",
    "             'TweetTokenizer': tweet_tokens }\n",
    "df = pd.DataFrame.from_dict(tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
