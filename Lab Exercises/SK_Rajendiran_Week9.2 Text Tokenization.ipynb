{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Name: Sathish Kumar Rajendiran\n",
    "    Task: Week 9: 9.2 Text Tokenization\n",
    "    Date: 8/26/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymongo library\n",
    "# !pip install pymongo\n",
    "# !pip install tweepy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sathishrajendiran/ist652-python'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "\n",
    "# standard library\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "# text tokenization\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#MongoDB libraries\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# csv, xls, pandas & json\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import xlrd\n",
    "\n",
    "\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  sample text\n",
    "text = '''I'll never fly Delta again!! My flight was\n",
    "supposed to leave MCO for ATL at 6:25pm on Saturday,\n",
    "March 26, however, due to severe weather, it was\n",
    "delayed until 8:12pm - no problem. At approx. 8pm we\n",
    "started boarding. We just sat there at the gate. No\n",
    "explanation etc. until about 9:30pm when the pilot said\n",
    "we were pushing away from the gate but wouldn't take\n",
    "off. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split text into words\n",
    "words = text.split()\n",
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK - Youre now connected to the MongoDB.\n",
      "\n",
      "MongoDB database: Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'bball')\n",
      "MongoDB collection:Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'bball'), 'bbcoll')\n"
     ]
    }
   ],
   "source": [
    "# Connecting to the database\n",
    "# Connection to Mongo DB\n",
    "try:\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    print ('Authentication OK - You''re now connected to the MongoDB.\\n')\n",
    "# use database named fbusers or create it if not there already\n",
    "    db = client.bball\n",
    "    # create collection named delta or create it if not there already\n",
    "    coll = db.bbcoll\n",
    "    print('MongoDB database: ' + str(db))\n",
    "    print('MongoDB collection:' + str(coll))\n",
    "\n",
    "except pymongo.errors.ConnectionFailure as e:\n",
    "    print ('Could not connect to MongoDB: %s' % e )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: database_names is deprecated. Use list_database_names instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['admin',\n",
       " 'bball',\n",
       " 'config',\n",
       " 'covid',\n",
       " 'disney',\n",
       " 'fbusers',\n",
       " 'local',\n",
       " 'peopledb',\n",
       " 'pytweetsdb',\n",
       " 'tweetsdb',\n",
       " 'usgs']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list database defined\n",
    "client.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: collection_names is deprecated. Use list_collection_names instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bbcoll']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('58d992de64a4f3e56d2db3c7'),\n",
       " 'user': {'profile_background_tile': True,\n",
       "  'friends_count': 217,\n",
       "  'profile_sidebar_fill_color': 'EFEFEF',\n",
       "  'id_str': '513196438',\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_link_color': 'F70808',\n",
       "  'followers_count': 256,\n",
       "  'location': '',\n",
       "  'protected': False,\n",
       "  'default_profile_image': False,\n",
       "  'contributors_enabled': False,\n",
       "  'favourites_count': 2187,\n",
       "  'profile_background_color': 'BFD0D9',\n",
       "  'statuses_count': 1104,\n",
       "  'id': 513196438,\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/513196438/1437359097',\n",
       "  'created_at': 'Sat Mar 03 13:54:37 +0000 2012',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/732514428051181568/OkOVa8Ia_normal.jpg',\n",
       "  'time_zone': 'Eastern Time (US & Canada)',\n",
       "  'follow_request_sent': None,\n",
       "  'listed_count': 1,\n",
       "  'utc_offset': -14400,\n",
       "  'lang': 'en',\n",
       "  'is_translator': False,\n",
       "  'name': 'Will',\n",
       "  'description': \"Do not worry about tomorrow, for tomorrow will worry about itself.-Matthew 6:34 HSC'20\",\n",
       "  'profile_use_background_image': True,\n",
       "  'verified': False,\n",
       "  'geo_enabled': True,\n",
       "  'profile_text_color': '333333',\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/732514428051181568/OkOVa8Ia_normal.jpg',\n",
       "  'entities': {'description': {'urls': []}},\n",
       "  'notifications': None,\n",
       "  'url': None,\n",
       "  'translator_type': 'none',\n",
       "  'has_extended_profile': True,\n",
       "  'default_profile': False,\n",
       "  'screen_name': 'frenchythe1st',\n",
       "  'following': None,\n",
       "  'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/850934042/26e0a43c1f821ac098571fb3de80944c.jpeg',\n",
       "  'profile_sidebar_border_color': 'FFFFFF',\n",
       "  'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/850934042/26e0a43c1f821ac098571fb3de80944c.jpeg'},\n",
       " 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       " 'favorited': False,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'text': 'RT @HowardWKYT: The final seconds of the Kentucky-North Carolina game was an emotional roller coaster for Big Blue fans. #marchmadness #WKY…',\n",
       " 'retweet_count': 9739,\n",
       " 'id': 8.464896333870899e+17,\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'created_at': 'Mon Mar 27 22:30:30 +0000 2017',\n",
       " 'is_quote_status': False,\n",
       " 'retweeted_status': {'user': {'profile_background_tile': False,\n",
       "   'friends_count': 573,\n",
       "   'profile_sidebar_fill_color': 'C0DFEC',\n",
       "   'id_str': '27059989',\n",
       "   'is_translation_enabled': False,\n",
       "   'profile_link_color': '0084B4',\n",
       "   'followers_count': 3330,\n",
       "   'location': 'Lexington, KY',\n",
       "   'protected': False,\n",
       "   'default_profile_image': False,\n",
       "   'contributors_enabled': False,\n",
       "   'favourites_count': 1231,\n",
       "   'profile_background_color': '000000',\n",
       "   'statuses_count': 9463,\n",
       "   'id': 27059989,\n",
       "   'profile_banner_url': 'https://pbs.twimg.com/profile_banners/27059989/1431999643',\n",
       "   'created_at': 'Fri Mar 27 18:09:16 +0000 2009',\n",
       "   'profile_image_url_https': 'https://pbs.twimg.com/profile_images/575376840304386049/atONJG3G_normal.jpeg',\n",
       "   'time_zone': 'Eastern Time (US & Canada)',\n",
       "   'follow_request_sent': None,\n",
       "   'listed_count': 77,\n",
       "   'utc_offset': -14400,\n",
       "   'lang': 'en',\n",
       "   'is_translator': False,\n",
       "   'name': 'Lee K. Howard',\n",
       "   'description': 'Sports Anchor/Reporter for CBS/FOX in Lexington Kentucky, providing sports news and my random rants and chants!',\n",
       "   'profile_use_background_image': True,\n",
       "   'verified': True,\n",
       "   'geo_enabled': True,\n",
       "   'profile_text_color': '333333',\n",
       "   'profile_image_url': 'http://pbs.twimg.com/profile_images/575376840304386049/atONJG3G_normal.jpeg',\n",
       "   'entities': {'description': {'urls': []},\n",
       "    'url': {'urls': [{'expanded_url': 'http://www.facebook.com/profile.php?id=100003241678454',\n",
       "       'display_url': 'facebook.com/profile.php?id…',\n",
       "       'indices': [0, 22],\n",
       "       'url': 'http://t.co/C2UFfLkjB1'}]}},\n",
       "   'notifications': None,\n",
       "   'url': 'http://t.co/C2UFfLkjB1',\n",
       "   'translator_type': 'none',\n",
       "   'has_extended_profile': False,\n",
       "   'default_profile': False,\n",
       "   'screen_name': 'HowardWKYT',\n",
       "   'following': None,\n",
       "   'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/596008542/6qfqqg0jndpp3su53ay5.jpeg',\n",
       "   'profile_sidebar_border_color': 'A8C7F7',\n",
       "   'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/596008542/6qfqqg0jndpp3su53ay5.jpeg'},\n",
       "  'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       "  'favorited': False,\n",
       "  'in_reply_to_user_id': None,\n",
       "  'text': 'The final seconds of the Kentucky-North Carolina game was an emotional roller coaster for Big Blue fans.… https://t.co/TPZ6PuXHxH',\n",
       "  'possibly_sensitive': False,\n",
       "  'retweet_count': 9739,\n",
       "  'id': 8.462142300261786e+17,\n",
       "  'in_reply_to_screen_name': None,\n",
       "  'created_at': 'Mon Mar 27 04:16:08 +0000 2017',\n",
       "  'is_quote_status': False,\n",
       "  'favorite_count': 12590,\n",
       "  'contributors': None,\n",
       "  'lang': 'en',\n",
       "  'in_reply_to_status_id_str': None,\n",
       "  'source': '<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>',\n",
       "  'in_reply_to_status_id': None,\n",
       "  'place': {'country': 'United States',\n",
       "   'contained_within': [],\n",
       "   'id': '6ffcf3b0b904bbcb',\n",
       "   'country_code': 'US',\n",
       "   'bounding_box': {'type': 'Polygon',\n",
       "    'coordinates': [[[-89.57151, 36.497129],\n",
       "      [-81.964971, 36.497129],\n",
       "      [-81.964971, 39.147359],\n",
       "      [-89.57151, 39.147359]]]},\n",
       "   'place_type': 'admin',\n",
       "   'full_name': 'Kentucky, USA',\n",
       "   'url': 'https://api.twitter.com/1.1/geo/id/6ffcf3b0b904bbcb.json',\n",
       "   'name': 'Kentucky',\n",
       "   'attributes': {}},\n",
       "  'entities': {'hashtags': [],\n",
       "   'urls': [{'expanded_url': 'https://twitter.com/i/web/status/846214230026178564',\n",
       "     'display_url': 'twitter.com/i/web/status/8…',\n",
       "     'indices': [106, 129],\n",
       "     'url': 'https://t.co/TPZ6PuXHxH'}],\n",
       "   'symbols': [],\n",
       "   'user_mentions': []},\n",
       "  'geo': None,\n",
       "  'truncated': True,\n",
       "  'coordinates': None,\n",
       "  'in_reply_to_user_id_str': None,\n",
       "  'retweeted': False,\n",
       "  'id_str': '846214230026178564'},\n",
       " 'favorite_count': 0,\n",
       " 'contributors': None,\n",
       " 'lang': 'en',\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
       " 'in_reply_to_status_id': None,\n",
       " 'place': None,\n",
       " 'entities': {'hashtags': [{'indices': [121, 134], 'text': 'marchmadness'}],\n",
       "  'urls': [],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [{'indices': [3, 14],\n",
       "    'name': 'Lee K. Howard',\n",
       "    'id': 27059989,\n",
       "    'id_str': '27059989',\n",
       "    'screen_name': 'HowardWKYT'}]},\n",
       " 'geo': None,\n",
       " 'truncated': False,\n",
       " 'coordinates': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'retweeted': False,\n",
       " 'id_str': '846489633387089920'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#search the first item from the collection\n",
    "coll.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs:  2000\n"
     ]
    }
   ],
   "source": [
    "#read through collections for tweets\n",
    "docs = coll.find()\n",
    "doclist = list(docs)\n",
    "msglist =[doc['text'] for doc in doclist if 'text' in doc.keys()]\n",
    "print('Number of docs: ',len(msglist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  45701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@',\n",
       " 'HowardWKYT',\n",
       " ':',\n",
       " 'The',\n",
       " 'final',\n",
       " 'seconds',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Kentucky-North',\n",
       " 'Carolina',\n",
       " 'game',\n",
       " 'was',\n",
       " 'an',\n",
       " 'emotional',\n",
       " 'roller',\n",
       " 'coaster',\n",
       " 'for',\n",
       " 'Big',\n",
       " 'Blue',\n",
       " 'fans',\n",
       " '.',\n",
       " '#',\n",
       " 'marchmadness',\n",
       " '#',\n",
       " 'WKY…',\n",
       " 'RT',\n",
       " '@',\n",
       " 'WhistleSports',\n",
       " ':',\n",
       " 'When',\n",
       " 'you',\n",
       " 'perfectly',\n",
       " 'time',\n",
       " 'the',\n",
       " '#',\n",
       " 'UNC',\n",
       " 'buzzer',\n",
       " 'beater',\n",
       " '😱🏀',\n",
       " '#',\n",
       " 'MarchMadness',\n",
       " '(',\n",
       " 'via',\n",
       " ':',\n",
       " '@',\n",
       " 'SamuelGrubbs1',\n",
       " ')',\n",
       " 'https',\n",
       " ':']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = [tok for msg in msglist for tok in nltk.word_tokenize(msg)]\n",
    "print('Number of tokens: ',len(all_tokens))\n",
    "#print top 5 tokens\n",
    "all_tokens[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 4245),\n",
       " (':', 2900),\n",
       " ('@', 2154),\n",
       " ('RT', 1543),\n",
       " ('https', 1224),\n",
       " ('marchmadness', 1124),\n",
       " ('the', 1092),\n",
       " ('.', 1049),\n",
       " ('MarchMadness', 928),\n",
       " ('of', 901),\n",
       " ('game', 669),\n",
       " ('Carolina', 668),\n",
       " ('for', 659),\n",
       " ('was', 645),\n",
       " ('fans', 644),\n",
       " ('The', 582),\n",
       " ('final', 526),\n",
       " ('an', 526),\n",
       " ('seconds', 515),\n",
       " ('Big', 510),\n",
       " ('HowardWKYT', 507),\n",
       " ('Kentucky-North', 507),\n",
       " ('emotional', 507),\n",
       " ('roller', 507),\n",
       " ('coaster', 507),\n",
       " ('Blue', 507),\n",
       " ('WKY…', 507),\n",
       " ('!', 492),\n",
       " ('to', 290),\n",
       " (',', 279)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgFD = nltk.FreqDist(all_tokens)\n",
    "msgFD.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " '@',\n",
       " 'howardwkyt',\n",
       " ':',\n",
       " 'the',\n",
       " 'final',\n",
       " 'seconds',\n",
       " 'of',\n",
       " 'the',\n",
       " 'kentucky-north',\n",
       " 'carolina',\n",
       " 'game',\n",
       " 'was',\n",
       " 'an',\n",
       " 'emotional',\n",
       " 'roller',\n",
       " 'coaster',\n",
       " 'for',\n",
       " 'big',\n",
       " 'blue',\n",
       " 'fans',\n",
       " '.',\n",
       " '#',\n",
       " 'marchmadness',\n",
       " '#',\n",
       " 'wky…',\n",
       " 'rt',\n",
       " '@',\n",
       " 'whistlesports',\n",
       " ':']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all tokens to lowercase\n",
    "all_tokens = [tok.lower() for msg in msglist for tok in nltk.word_tokenize(msg)]\n",
    "all_tokens[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords:  179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
    "print('Number of stopwords: ',len(nltk_stopwords))\n",
    "nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_filter(w):\n",
    "    pattern = re.compile('^[^a-z]+$')\n",
    "    if (pattern.match(w)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens:  29855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['HowardWKYT',\n",
       " 'The',\n",
       " 'final',\n",
       " 'seconds',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Kentucky-North',\n",
       " 'Carolina',\n",
       " 'game',\n",
       " 'was',\n",
       " 'an',\n",
       " 'emotional',\n",
       " 'roller',\n",
       " 'coaster',\n",
       " 'for',\n",
       " 'Big',\n",
       " 'Blue',\n",
       " 'fans',\n",
       " 'marchmadness',\n",
       " 'WhistleSports',\n",
       " 'When',\n",
       " 'you',\n",
       " 'perfectly',\n",
       " 'time',\n",
       " 'the',\n",
       " 'buzzer',\n",
       " 'beater',\n",
       " 'MarchMadness',\n",
       " 'via',\n",
       " 'SamuelGrubbs1',\n",
       " 'https',\n",
       " '//t.co/Ol2ibpZjB4',\n",
       " 'HowardWKYT',\n",
       " 'The',\n",
       " 'final',\n",
       " 'seconds',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Kentucky-North',\n",
       " 'Carolina',\n",
       " 'game',\n",
       " 'was',\n",
       " 'an',\n",
       " 'emotional',\n",
       " 'roller',\n",
       " 'coaster',\n",
       " 'for',\n",
       " 'Big',\n",
       " 'Blue',\n",
       " 'fans',\n",
       " 'marchmadness',\n",
       " 'BleacherReport',\n",
       " 'And',\n",
       " 'then',\n",
       " 'there',\n",
       " 'were',\n",
       " 'four',\n",
       " 'MarchMadness',\n",
       " 'https',\n",
       " '//t.co/0MbxpgAuUC',\n",
       " 'mycarolinastdnt',\n",
       " 'if',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'be',\n",
       " 'cheering',\n",
       " 'on',\n",
       " 'GamecockWBB',\n",
       " 'and',\n",
       " 'dawnstaley',\n",
       " 'tonight',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'Go',\n",
       " 'Gamecocks',\n",
       " 'MarchMadness',\n",
       " 'HowardWKYT',\n",
       " 'The',\n",
       " 'final',\n",
       " 'seconds',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Kentucky-North',\n",
       " 'Carolina',\n",
       " 'game',\n",
       " 'was',\n",
       " 'an',\n",
       " 'emotional',\n",
       " 'roller',\n",
       " 'coaster',\n",
       " 'for',\n",
       " 'Big',\n",
       " 'Blue',\n",
       " 'fans',\n",
       " 'marchmadness',\n",
       " 'marchmadness',\n",
       " 'For',\n",
       " 'Kentucky',\n",
       " 'and',\n",
       " 'North']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list = [tok for tok in all_tokens if not alpha_filter(tok)]\n",
    "print('Number of tokens: ',len(token_list))\n",
    "token_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marchmadness 2071\n",
      "the 1674\n",
      "rt 1548\n",
      "https 1224\n",
      "of 908\n",
      "for 781\n",
      "game 675\n",
      "carolina 669\n",
      "was 646\n",
      "fans 644\n",
      "final 571\n",
      "an 530\n",
      "big 522\n",
      "seconds 520\n",
      "howardwkyt 507\n",
      "kentucky-north 507\n",
      "emotional 507\n",
      "roller 507\n",
      "coaster 507\n",
      "blue 507\n",
      "wky… 507\n",
      "and 383\n",
      "to 293\n",
      "a 283\n",
      "'s 236\n",
      "four 208\n",
      "you 205\n",
      "is 182\n",
      "bleacherreport 180\n",
      "in 180\n"
     ]
    }
   ],
   "source": [
    "# Top 30 words by frequency\n",
    "\n",
    "msgFD = nltk.FreqDist(token_list)\n",
    "top_words = msgFD.most_common(30)\n",
    "for word, freq in top_words:\n",
    "    print(word, freq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https', '://', 't', '.', 'co', '/', '9z2J3P33Uc']\n",
      "['laugh', '/', 'cry']\n",
      "['😬😭😓🤢🙄😱']\n",
      "['world', \"'\", 's', 'problems']\n",
      "['@', 'datageneral']\n",
      "['It', \"'\", 's', 'interesting']\n",
      "['don', \"'\", 't', 'spell', 'my', 'name', 'right']\n",
      "['all', '-', 'nighter']\n"
     ]
    }
   ],
   "source": [
    "#WORDPUNCTTOKENIZER splits all punctuations into separate tokens. \n",
    "\n",
    "\n",
    "compare_list = ['https://t.co/9z2J3P33Uc',\n",
    "               'laugh/cry',\n",
    "               '😬😭😓🤢🙄😱',\n",
    "               \"world's problems\",\n",
    "               \"@datageneral\",\n",
    "                \"It's interesting\",\n",
    "               \"don't spell my name right\",\n",
    "               'all-nighter']\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "punct_tokenizer = WordPunctTokenizer()\n",
    "punct_tokens = []\n",
    "for sent in compare_list:\n",
    "    print(punct_tokenizer.tokenize(sent))\n",
    "    punct_tokens.append(punct_tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https', ':', '//t.co/9z2J3P33Uc']\n",
      "['laugh/cry']\n",
      "['😬😭😓🤢🙄😱']\n",
      "['world', \"'s\", 'problems']\n",
      "['@', 'datageneral']\n",
      "['It', \"'s\", 'interesting']\n",
      "['do', \"n't\", 'spell', 'my', 'name', 'right']\n",
      "['all-nighter']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = []\n",
    "for sent in compare_list:\n",
    "    print(word_tokenize(sent))\n",
    "    word_tokens.append(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https', 't', 'co', '9z2J3P33Uc']\n",
      "['laugh', 'cry']\n",
      "[]\n",
      "[\"world's\", 'problems']\n",
      "['datageneral']\n",
      "[\"It's\", 'interesting']\n",
      "[\"don't\", 'spell', 'my', 'name', 'right']\n",
      "['all', 'nighter']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "match_tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "match_tokens = []\n",
    "for sent in compare_list:   \n",
    "    print(match_tokenizer.tokenize(sent))\n",
    "    match_tokens.append(match_tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://t.co/9z2J3P33Uc']\n",
      "['laugh', '/', 'cry']\n",
      "['😬', '😭', '😓', '🤢', '🙄', '😱']\n",
      "[\"world's\", 'problems']\n",
      "['@datageneral']\n",
      "[\"It's\", 'interesting']\n",
      "[\"don't\", 'spell', 'my', 'name', 'right']\n",
      "['all-nighter']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = []\n",
    "for sent in compare_list:\n",
    "    print(tweet_tokenizer.tokenize(sent))\n",
    "    tweet_tokens.append(tweet_tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_tokenize</th>\n",
       "      <th>WordPunctTokenize</th>\n",
       "      <th>RegrexTokenizer for matching</th>\n",
       "      <th>RegrexTokenizer for white space</th>\n",
       "      <th>TweetTokenizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[https, :, //t.co/9z2J3P33Uc]</td>\n",
       "      <td>[https, ://, t, ., co, /, 9z2J3P33Uc]</td>\n",
       "      <td>[https, t, co, 9z2J3P33Uc]</td>\n",
       "      <td>[https://t.co/9z2J3P33Uc]</td>\n",
       "      <td>[https://t.co/9z2J3P33Uc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[laugh/cry]</td>\n",
       "      <td>[laugh, /, cry]</td>\n",
       "      <td>[laugh, cry]</td>\n",
       "      <td>[laugh/cry]</td>\n",
       "      <td>[laugh, /, cry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[😬😭😓🤢🙄😱]</td>\n",
       "      <td>[😬😭😓🤢🙄😱]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[😬😭😓🤢🙄😱]</td>\n",
       "      <td>[😬, 😭, 😓, 🤢, 🙄, 😱]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[world, 's, problems]</td>\n",
       "      <td>[world, ', s, problems]</td>\n",
       "      <td>[world's, problems]</td>\n",
       "      <td>[world's, problems]</td>\n",
       "      <td>[world's, problems]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[@, datageneral]</td>\n",
       "      <td>[@, datageneral]</td>\n",
       "      <td>[datageneral]</td>\n",
       "      <td>[@datageneral]</td>\n",
       "      <td>[@datageneral]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[It, 's, interesting]</td>\n",
       "      <td>[It, ', s, interesting]</td>\n",
       "      <td>[It's, interesting]</td>\n",
       "      <td>[It's, interesting]</td>\n",
       "      <td>[It's, interesting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[do, n't, spell, my, name, right]</td>\n",
       "      <td>[don, ', t, spell, my, name, right]</td>\n",
       "      <td>[don't, spell, my, name, right]</td>\n",
       "      <td>[don't, spell, my, name, right]</td>\n",
       "      <td>[don't, spell, my, name, right]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[all-nighter]</td>\n",
       "      <td>[all, -, nighter]</td>\n",
       "      <td>[all, nighter]</td>\n",
       "      <td>[all-nighter]</td>\n",
       "      <td>[all-nighter]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       word_tokenize                      WordPunctTokenize  \\\n",
       "0      [https, :, //t.co/9z2J3P33Uc]  [https, ://, t, ., co, /, 9z2J3P33Uc]   \n",
       "1                        [laugh/cry]                        [laugh, /, cry]   \n",
       "2                           [😬😭😓🤢🙄😱]                               [😬😭😓🤢🙄😱]   \n",
       "3              [world, 's, problems]                [world, ', s, problems]   \n",
       "4                   [@, datageneral]                       [@, datageneral]   \n",
       "5              [It, 's, interesting]                [It, ', s, interesting]   \n",
       "6  [do, n't, spell, my, name, right]    [don, ', t, spell, my, name, right]   \n",
       "7                      [all-nighter]                      [all, -, nighter]   \n",
       "\n",
       "      RegrexTokenizer for matching  RegrexTokenizer for white space  \\\n",
       "0       [https, t, co, 9z2J3P33Uc]        [https://t.co/9z2J3P33Uc]   \n",
       "1                     [laugh, cry]                      [laugh/cry]   \n",
       "2                               []                         [😬😭😓🤢🙄😱]   \n",
       "3              [world's, problems]              [world's, problems]   \n",
       "4                    [datageneral]                   [@datageneral]   \n",
       "5              [It's, interesting]              [It's, interesting]   \n",
       "6  [don't, spell, my, name, right]  [don't, spell, my, name, right]   \n",
       "7                   [all, nighter]                    [all-nighter]   \n",
       "\n",
       "                    TweetTokenizer  \n",
       "0        [https://t.co/9z2J3P33Uc]  \n",
       "1                  [laugh, /, cry]  \n",
       "2               [😬, 😭, 😓, 🤢, 🙄, 😱]  \n",
       "3              [world's, problems]  \n",
       "4                   [@datageneral]  \n",
       "5              [It's, interesting]  \n",
       "6  [don't, spell, my, name, right]  \n",
       "7                    [all-nighter]  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizers = {'word_tokenize': word_tokens,\n",
    "             'WordPunctTokenize':punct_tokens,\n",
    "             'RegrexTokenizer for matching':match_tokens,\n",
    "             'RegrexTokenizer for white space': space_tokens,\n",
    "             'TweetTokenizer': tweet_tokens }\n",
    "df = pd.DataFrame.from_dict(tokenizers)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural language processing nlp is',\n",
       " 'language processing nlp is an',\n",
       " 'processing nlp is an area',\n",
       " 'nlp is an area of\\n',\n",
       " 'is an area of\\n computer',\n",
       " 'an area of\\n computer science',\n",
       " 'area of\\n computer science and',\n",
       " 'of\\n computer science and artificial',\n",
       " 'computer science and artificial intelligence\\n',\n",
       " 'science and artificial intelligence\\n concerned',\n",
       " 'and artificial intelligence\\n concerned with',\n",
       " 'artificial intelligence\\n concerned with the',\n",
       " 'intelligence\\n concerned with the interactions',\n",
       " 'concerned with the interactions between',\n",
       " 'with the interactions between computers\\n',\n",
       " 'the interactions between computers\\n and',\n",
       " 'interactions between computers\\n and human',\n",
       " 'between computers\\n and human natural',\n",
       " 'computers\\n and human natural languages']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"\"\"natural-language processing (NLP) is an area of\n",
    "    computer science and artificial intelligence\n",
    "    concerned with the interactions between computers\n",
    "    and human (natural) languages.\"\"\"\n",
    "\n",
    "generate_ngrams(s, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('natural', 'language', 'processing', 'nlp', 'is')\n",
      "('language', 'processing', 'nlp', 'is', 'an')\n",
      "('processing', 'nlp', 'is', 'an', 'area')\n",
      "('nlp', 'is', 'an', 'area', 'of\\n')\n",
      "('is', 'an', 'area', 'of\\n', 'computer')\n",
      "('an', 'area', 'of\\n', 'computer', 'science')\n",
      "('area', 'of\\n', 'computer', 'science', 'and')\n",
      "('of\\n', 'computer', 'science', 'and', 'artificial')\n",
      "('computer', 'science', 'and', 'artificial', 'intelligence\\n')\n",
      "('science', 'and', 'artificial', 'intelligence\\n', 'concerned')\n",
      "('and', 'artificial', 'intelligence\\n', 'concerned', 'with')\n",
      "('artificial', 'intelligence\\n', 'concerned', 'with', 'the')\n",
      "('intelligence\\n', 'concerned', 'with', 'the', 'interactions')\n",
      "('concerned', 'with', 'the', 'interactions', 'between')\n",
      "('with', 'the', 'interactions', 'between', 'computers\\n')\n",
      "('the', 'interactions', 'between', 'computers\\n', 'and')\n",
      "('interactions', 'between', 'computers\\n', 'and', 'human')\n",
      "('between', 'computers\\n', 'and', 'human', 'natural')\n",
      "('computers\\n', 'and', 'human', 'natural', 'languages')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "s = s.lower()\n",
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "output = list(ngrams(tokens, 5))\n",
    "\n",
    "#%%\n",
    "\n",
    "for x in output:\n",
    "    print(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
