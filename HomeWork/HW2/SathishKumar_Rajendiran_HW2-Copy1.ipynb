{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ***********************************************************************************\n",
    "#      Python Version check for libraries compatibility\n",
    "# ***********************************************************************************\n",
    "\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Name: Sathish Kumar Rajendiran\n",
    "    Task: Homework 2: Semistructured Data\n",
    "    Date: 8/5/2020\n",
    " \n",
    "     Semistructured Data Processing\n",
    "         The main outline of your assignment is to write a program that will read in JSON formatted data from a Mongo DB collection or from a file. This will be in a format that is structured with lines of data representing one type of unit, for example, one tweet for Twitter or one post from Facebook. Your program will contain the data as lists of JSON structures, which are just Python dictionaries and lists. Your program may also contain pandas dataframes for processed data.\n",
    "         The program will do some processing to collect data from some of the fields that will answer one or more questions as described below, and write a file with the data suitable for answering each question. Remember that some fields may be optional or have null values, so you may need to test for those conditions. Graphing is definitely optional.\n",
    "         \n",
    "    Tweets\n",
    "    Retweets\n",
    "    Likes\n",
    "    Direct messages\n",
    "    Favorites\n",
    "    Trends\n",
    "    Media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ***********************************************************************************\n",
    "#      package installation\n",
    "# ***********************************************************************************\n",
    "\n",
    "# !pip install tweepy\n",
    "# !pip install tweet-preprocessor\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/sathishrajendiran/ist652-python/HW2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ***********************************************************************************\n",
    "#      import libraries\n",
    "# ***********************************************************************************\n",
    "\n",
    "# standard library\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import timeit\n",
    "\n",
    "# csv, xls, pandas & json\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import xlrd\n",
    "import numpy as np\n",
    "\n",
    "#twitter libraries\n",
    "import tweepy\n",
    "from tweepy import StreamListener\n",
    "from tweepy import Stream\n",
    "import preprocessor as p\n",
    "    \n",
    "# from tweet-preprocessor import clean,tokenize,parse\n",
    "\n",
    "#MongoDB libraries\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "print('Libraries imported successfully!\\n')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has been processed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ***********************************************************************************\n",
    "#       Twitter credentials file\n",
    "# ***********************************************************************************\n",
    "\n",
    "# ls *.xls\n",
    "\n",
    "#create dummy dictionary\n",
    "di = {}\n",
    "# define file name\n",
    "infile = 'tw_credentials.xls'\n",
    "\n",
    "# Working with file\n",
    "try: \n",
    "    df = pd.read_excel(infile, encoding='utf-16')\n",
    "    di = df.to_dict()\n",
    "    print(\"data has been processed \\n\")\n",
    "except:\n",
    "    print(\"Is the file in correct directory?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search words ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ***********************************************************************************\n",
    "#       Twitter feeds keywords collection\n",
    "# ***********************************************************************************\n",
    "\n",
    "\n",
    "#  words = ['#LA', '#LosAngeles', '#LAtraffic', '#accidents','#hollywood',\n",
    "#         '#LAFD','#Wildfire','#LAHeatWave','#STREETCLOSURE','#car']\n",
    "\n",
    "words = ['#LA', '#LosAngeles', '#LAtraffic','#LAFD','#LASTREETCLOSURE']  # key words\n",
    "\n",
    "lang=['en']  # language - english\n",
    "\n",
    "print(\"search words ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streaming api listening...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ***********************************************************************************\n",
    "#       Define Twitter streaming feed into MongoDB collection\n",
    "# ***********************************************************************************\n",
    "\n",
    "class TwitterStream(tweepy.StreamListener):\n",
    "    \n",
    "        def on_connect(self):\n",
    "        # Function called to connect to the Twitter Streaming API\n",
    "            print('\\nTweets follow...')\n",
    "\n",
    "        def on_status(self,status):\n",
    "            if status.retweeted_status:\n",
    "                return\n",
    "            print(status.text)\n",
    "\n",
    "        def on_error(self,status_code):\n",
    "            print('Encounted Streaming error('',status_code,'')')\n",
    "            return False\n",
    "\n",
    "        def on_data(self, data): \n",
    "            try:\n",
    "\n",
    "                datajson = json.loads(data)\n",
    "                tweet_message = datajson['text']\n",
    "                print('\\n',tweet_message)\n",
    "\n",
    "                tweetscoll.insert(datajson)\n",
    "\n",
    "            except Exception as e:\n",
    "                   print(e)\n",
    "print(\"streaming api listening...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK - Youre now connected to the Twitter API.\n",
      "\n",
      "Authentication OK - Youre now connected to the MongoDB.\n",
      "\n",
      "MongoDB database: Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'tweetsdb')\n",
      "MongoDB collection:Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'tweetsdb'), 'tweets')\n",
      "\n",
      "Start Streaming...\n",
      "Keywords:['#LA', '#LosAngeles', '#LAtraffic', '#LAFD', '#LASTREETCLOSURE']\n",
      "Languages:['en']\n",
      "\n",
      "Tweets follow...\n",
      "\n",
      "Stopped.\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ***********************************************************************************\n",
    "#       Main function to stream Tweets into MongoDB collection\n",
    "# ***********************************************************************************\n",
    "\n",
    "\n",
    "#main function\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "        #Twitter connection details \n",
    "\n",
    "        #assign it to variables\n",
    "        for key,val in di.items():\n",
    "            consumer_key = val[0]\n",
    "            consumer_secret = val[1]\n",
    "            access_token = val[2]\n",
    "            access_secret = val[3]\n",
    "\n",
    "        # test authentication\n",
    "        try:\n",
    "            auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "            auth.set_access_token(access_token, access_secret)\n",
    "            api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "            api.verify_credentials()\n",
    "            print('Authentication OK - You''re now connected to the Twitter API.\\n')\n",
    "\n",
    "        except:\n",
    "            print('Error during authentication')\n",
    "\n",
    "        # Connection to Mongo DB\n",
    "        try:\n",
    "            client = MongoClient('localhost', 27017)\n",
    "            print ('Authentication OK - You''re now connected to the MongoDB.\\n')\n",
    "\n",
    "            # use database named usgs or create it if not there already\n",
    "            twdb = client.tweetsdb\n",
    "            # create collection named earthquakes or create it if not there already\n",
    "            tweetscoll = twdb.tweets\n",
    "\n",
    "            print('MongoDB database: ' + str(twdb))\n",
    "            print('MongoDB collection:' + str(tweetscoll))\n",
    "            \n",
    "        except pymongo.errors.ConnectionFailure as e:\n",
    "            print ('Could not connect to MongoDB: %s' % e )\n",
    "\n",
    "\n",
    "            \n",
    "        #intialize Stream\n",
    "        try:\n",
    "            print('\\nStart Streaming...')\n",
    "            print('Keywords:'  + str(words))\n",
    "            print('Languages:'  + str(lang))\n",
    "            listener = TwitterStream(api=tweepy.API(wait_on_rate_limit=True)) \n",
    "            streamer = tweepy.Stream(auth=auth, listener=listener,tweet_mode='extended')\n",
    "            # print('Date Since:'  + str(date_since))\n",
    "            streamer.filter(track=words,languages=lang,encoding='utf8',follow=None, )\n",
    "            \n",
    "        except KeyboardInterrupt as e :\n",
    "            print(\"\\nStopped.\")\n",
    "            \n",
    "        finally:\n",
    "            print('\\nDone.')\n",
    "            streamer.disconnect()\n",
    "            client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documents:  277283\n"
     ]
    }
   ],
   "source": [
    "#print the number of docs from db\n",
    "print('Total Number of Documents: ',tweetscoll.count_documents({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5f39fe371a3bc61df03bfac4'),\n",
       " 'created_at': 'Mon Aug 17 03:49:06 +0000 2020',\n",
       " 'id': 1295205989478731781,\n",
       " 'id_str': '1295205989478731781',\n",
       " 'text': 'Hurricane Awareness: Zephyr Insurance \\n\\nREAD MORE: https://t.co/SEhxK4bwyi\\n\\n#Accidents #Claims #DisasterMitigation… https://t.co/ew7ylyeHCz',\n",
       " 'display_text_range': [0, 140],\n",
       " 'source': '<a href=\"https://www.blog.iammarketingmedia.com\" rel=\"nofollow\">IAMBLOG2TWITTER</a>',\n",
       " 'truncated': True,\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'user': {'id': 226310002,\n",
       "  'id_str': '226310002',\n",
       "  'name': 'IAM Platform',\n",
       "  'screen_name': 'IAM__Network',\n",
       "  'location': 'Worldwide',\n",
       "  'url': 'https://www.iammarketingmedia.com',\n",
       "  'description': 'Curation | Tools | Tips | Services\\n\\nIAM Platform powers IAM Network:\\n\\nGO: http://bit.ly/2Ywsbg8\\n\\nBlog | Social | Podcast | Code Trove',\n",
       "  'translator_type': 'none',\n",
       "  'protected': False,\n",
       "  'verified': False,\n",
       "  'followers_count': 18016,\n",
       "  'friends_count': 14938,\n",
       "  'listed_count': 3290,\n",
       "  'favourites_count': 65467,\n",
       "  'statuses_count': 778665,\n",
       "  'created_at': 'Mon Dec 13 21:24:29 +0000 2010',\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': False,\n",
       "  'lang': None,\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'profile_background_color': '94D487',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_tile': False,\n",
       "  'profile_link_color': '3366CC',\n",
       "  'profile_sidebar_border_color': 'FFFFFF',\n",
       "  'profile_sidebar_fill_color': 'DDEEF6',\n",
       "  'profile_text_color': '333333',\n",
       "  'profile_use_background_image': True,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/701708113653669888/Nzm67hhC_normal.png',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/701708113653669888/Nzm67hhC_normal.png',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/226310002/1584072260',\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'following': None,\n",
       "  'follow_request_sent': None,\n",
       "  'notifications': None},\n",
       " 'geo': None,\n",
       " 'coordinates': None,\n",
       " 'place': None,\n",
       " 'contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'extended_tweet': {'full_text': 'Hurricane Awareness: Zephyr Insurance \\n\\nREAD MORE: https://t.co/SEhxK4bwyi\\n\\n#Accidents #Claims #DisasterMitigation #Insurance #InsuranceTechnology #InsurTech #Points #RiskMitigation #Technology~ https://t.co/kCrl2YxHfK',\n",
       "  'display_text_range': [0, 194],\n",
       "  'entities': {'hashtags': [{'text': 'Accidents', 'indices': [76, 86]},\n",
       "    {'text': 'Claims', 'indices': [87, 94]},\n",
       "    {'text': 'DisasterMitigation', 'indices': [95, 114]},\n",
       "    {'text': 'Insurance', 'indices': [115, 125]},\n",
       "    {'text': 'InsuranceTechnology', 'indices': [126, 146]},\n",
       "    {'text': 'InsurTech', 'indices': [147, 157]},\n",
       "    {'text': 'Points', 'indices': [158, 165]},\n",
       "    {'text': 'RiskMitigation', 'indices': [166, 181]},\n",
       "    {'text': 'Technology', 'indices': [182, 193]}],\n",
       "   'urls': [{'url': 'https://t.co/SEhxK4bwyi',\n",
       "     'expanded_url': 'https://blog.iammarketingmedia.com/hurricane-awareness-zephyr-insurance/?utm_campaign=twitter&utm_medium=twitter&utm_source=twitter',\n",
       "     'display_url': 'blog.iammarketingmedia.com/hurricane-awar…',\n",
       "     'indices': [51, 74]}],\n",
       "   'user_mentions': [],\n",
       "   'symbols': [],\n",
       "   'media': [{'id': 1295205987209621505,\n",
       "     'id_str': '1295205987209621505',\n",
       "     'indices': [195, 218],\n",
       "     'media_url': 'http://pbs.twimg.com/media/Efl--6qWsAEpwkT.jpg',\n",
       "     'media_url_https': 'https://pbs.twimg.com/media/Efl--6qWsAEpwkT.jpg',\n",
       "     'url': 'https://t.co/kCrl2YxHfK',\n",
       "     'display_url': 'pic.twitter.com/kCrl2YxHfK',\n",
       "     'expanded_url': 'https://twitter.com/IAM__Network/status/1295205989478731781/photo/1',\n",
       "     'type': 'photo',\n",
       "     'sizes': {'small': {'w': 448, 'h': 252, 'resize': 'fit'},\n",
       "      'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "      'medium': {'w': 448, 'h': 252, 'resize': 'fit'},\n",
       "      'large': {'w': 448, 'h': 252, 'resize': 'fit'}}}]},\n",
       "  'extended_entities': {'media': [{'id': 1295205987209621505,\n",
       "     'id_str': '1295205987209621505',\n",
       "     'indices': [195, 218],\n",
       "     'media_url': 'http://pbs.twimg.com/media/Efl--6qWsAEpwkT.jpg',\n",
       "     'media_url_https': 'https://pbs.twimg.com/media/Efl--6qWsAEpwkT.jpg',\n",
       "     'url': 'https://t.co/kCrl2YxHfK',\n",
       "     'display_url': 'pic.twitter.com/kCrl2YxHfK',\n",
       "     'expanded_url': 'https://twitter.com/IAM__Network/status/1295205989478731781/photo/1',\n",
       "     'type': 'photo',\n",
       "     'sizes': {'small': {'w': 448, 'h': 252, 'resize': 'fit'},\n",
       "      'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "      'medium': {'w': 448, 'h': 252, 'resize': 'fit'},\n",
       "      'large': {'w': 448, 'h': 252, 'resize': 'fit'}}}]}},\n",
       " 'quote_count': 0,\n",
       " 'reply_count': 0,\n",
       " 'retweet_count': 0,\n",
       " 'favorite_count': 0,\n",
       " 'entities': {'hashtags': [{'text': 'Accidents', 'indices': [76, 86]},\n",
       "   {'text': 'Claims', 'indices': [87, 94]},\n",
       "   {'text': 'DisasterMitigation', 'indices': [95, 114]}],\n",
       "  'urls': [{'url': 'https://t.co/SEhxK4bwyi',\n",
       "    'expanded_url': 'https://blog.iammarketingmedia.com/hurricane-awareness-zephyr-insurance/?utm_campaign=twitter&utm_medium=twitter&utm_source=twitter',\n",
       "    'display_url': 'blog.iammarketingmedia.com/hurricane-awar…',\n",
       "    'indices': [51, 74]},\n",
       "   {'url': 'https://t.co/ew7ylyeHCz',\n",
       "    'expanded_url': 'https://twitter.com/i/web/status/1295205989478731781',\n",
       "    'display_url': 'twitter.com/i/web/status/1…',\n",
       "    'indices': [116, 139]}],\n",
       "  'user_mentions': [],\n",
       "  'symbols': []},\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'possibly_sensitive': False,\n",
       " 'filter_level': 'low',\n",
       " 'lang': 'en',\n",
       " 'timestamp_ms': '1597636146312'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#search the first item from the collection\n",
    "tweetscoll.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for doc in tweetscoll.find().limit(2):\n",
    "#      print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The start time is : 411.482138406\n",
      "Total Number of Documents Processed:  277283\n",
      "The time difference is : 58.30324089200002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ***********************************************************************************\n",
    "#       load data from MongoDB collection to python list object\n",
    "# ***********************************************************************************\n",
    "\n",
    "starttime = timeit.default_timer()\n",
    "print(\"The start time is :\",starttime)\n",
    "tw_list =[]\n",
    "# results = tweetscoll.find().limit(5) #limit to 5 items\n",
    "results = tweetscoll.find()\n",
    "\n",
    "for result in results:\n",
    "    id = result['id']\n",
    "    id_str = result['id_str']\n",
    "    user = result['user']['name']\n",
    "    source = result['source']\n",
    "    followers = result['user']['followers_count']\n",
    "    retweets = result['retweet_count']\n",
    "    coords = result['coordinates']\n",
    "    bg_color = result['user']['profile_background_color']\n",
    "    unix_time_mil = result['timestamp_ms'] # select unix timestamp in milliseconds\n",
    "    unix_time = int(unix_time_mil) / 1000     # convert to unix in seconds\n",
    "    datets = datetime.fromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    if (result['place'] is None):\n",
    "        place = result['place']\n",
    "    else:\n",
    "        place = result['place']['full_name']\n",
    "    if (result['truncated']==True):\n",
    "        text = result['extended_tweet']['full_text']\n",
    "    else:\n",
    "        text = result['text']\n",
    "    try:\n",
    "        sensitivity = result['possibly_sensitive']\n",
    "    except KeyError:\n",
    "        sensitivity =''\n",
    "    tw_list.append([id,id_str,user,source,followers,retweets,coords,bg_color,datets,text])\n",
    "\n",
    "print('Total Number of Documents Processed: ',len(tw_list))\n",
    "\n",
    "print(\"The time difference is :\", timeit.default_timer() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ***********************************************************************************\n",
    "#       load data from python list into Pandas Dataframe\n",
    "# ***********************************************************************************\n",
    "\n",
    "\n",
    "#define column names\n",
    "ColNames = ['id','id_str','user','source','followers','retweets','coords','bg_color','datets','text']\n",
    "\n",
    "# Show all columns and do not truncate in the data frame\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "tweetsDF = pd.DataFrame(tw_list,columns=ColNames)\n",
    "\n",
    "print('Total Number of rows Processed: ',len(tweetsDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze Dataframe  - metadata\n",
    "tweetsDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze Dataframe - top 5 rows\n",
    "\n",
    "tweetsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze Dataframe - shape\n",
    "tweetsDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ***********************************************************************************\n",
    "#       Dataframe - Data Type conversion and Creation of Calender fields\n",
    "# ***********************************************************************************\n",
    "\n",
    "\n",
    "#convert datets values to float\n",
    "tweetsDF['datets'] = tweetsDF['datets'].astype('datetime64[ns]')\n",
    "\n",
    "#derive other calender items from date.today()\n",
    "# tweetsDF['datets'].unique()\n",
    "\n",
    "tweetsDF['date'] = tweetsDF['datets'].dt.date\n",
    "tweetsDF['year'] = tweetsDF['datets'].dt.year\n",
    "tweetsDF['month'] = tweetsDF['datets'].dt.month\n",
    "tweetsDF['monthday'] = tweetsDF['datets'].dt.day\n",
    "tweetsDF['weekday'] = tweetsDF['datets'].dt.weekday\n",
    "tweetsDF['dayname'] = tweetsDF['datets'].dt.day_name()\n",
    "tweetsDF['monthname'] = tweetsDF['datets'].dt.month_name()\n",
    "tweetsDF['hour'] = tweetsDF['datets'].dt.hour\n",
    "tweetsDF['minute'] = tweetsDF['datets'].dt.minute\n",
    "tweetsDF['secs'] = tweetsDF['datets'].dt.second\n",
    "\n",
    "tweetsDF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweetsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a another dataframe for further analysis\n",
    "NewtweetsDF = pd.DataFrame()\n",
    "NewtweetsDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewtweetsDF=tweetsDF\n",
    "NewtweetsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewtweetsDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #HappyEmoticons\n",
    "# emoticons_happy = set([\n",
    "#     ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "#     ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "#     '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "#     'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "#     '<3'\n",
    "#     ])\n",
    "\n",
    "# # Sad Emoticons\n",
    "# emoticons_sad = set([\n",
    "#     ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "#     ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "#     ':c', ':{', '>:\\\\', ';('\n",
    "#     ])\n",
    "\n",
    "# #Emoji patterns\n",
    "# emoji_pattern = re.compile(\"[\"\n",
    "#          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#          u\"\\U00002702-\\U000027B0\"\n",
    "#          u\"\\U000024C2-\\U0001F251\"\n",
    "#          \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# #combine sad and happy emoticons\n",
    "# emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NewtweetsDF['text'] = re.sub(r':', '', str(NewtweetsDF['text']))\n",
    "# NewtweetsDF['text'] = re.sub(r'‚Ä¶', '', str(NewtweetsDF['text']))\n",
    "# NewtweetsDF['text'] = re.sub(r'[^\\x00-\\x7F]+',' ', str(NewtweetsDF['text']))\n",
    "# NewtweetsDF['text'] = emoji_pattern.sub(r'', str(NewtweetsDF['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter_words =['los angeles', 'latraffic', 'california', 'fire']\n",
    "\n",
    "# def matcher(x):\n",
    "#     for i in filter_words:\n",
    "#         if i.lower() in x.lower():\n",
    "#             return i\n",
    "#     else:\n",
    "#         return np.nan\n",
    "\n",
    "# tweetsDF['text'] = tweetsDF['text'].apply(matcher)\n",
    "\n",
    "la_tweets = NewtweetsDF[NewtweetsDF['text'].str.contains('los angeles|latraffic|california|fire')]\n",
    "la_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewtweetsDF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewtweetsDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data spread- Cleanup tweet and username fields using preprocessor api library\n",
    "starttime = timeit.default_timer()\n",
    "print(\"The start time is :\",starttime)\n",
    "tempDF1 = tweetsDF[tweetsDF['text'].str.startswith('RT')==True]\n",
    "# retweetsDF.info() #184624\n",
    "tempDF1 = tempDF1.reset_index()\n",
    "print(\"The time difference is :\", timeit.default_timer() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = timeit.default_timer()\n",
    "print(\"The start time is :\",starttime)\n",
    "tempDF1['tweet']=''\n",
    "tempDF1['username']=''\n",
    "for i in tempDF1.index:\n",
    "    tempDF1['tweet'][i]= p.clean(tempDF1['text'].iloc[i])\n",
    "    tempDF1['username'][i]= p.clean(tempDF1['user'].iloc[i])\n",
    "    i += 1\n",
    "    print(\"i\", i)\n",
    "print(\"items updated already!\")\n",
    "print(\"The time difference is :\", timeit.default_timer() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data spread- Cleanup tweet and username fields using preprocessor api library\n",
    "starttime = timeit.default_timer()\n",
    "print(\"The start time is :\",starttime)\n",
    "tempDF2 = tweetsDF[tweetsDF['text'].str.startswith('RT')==False]\n",
    "tempDF2 = tempDF2.reset_index()\n",
    "print(\"The time difference is :\", timeit.default_timer() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup corrupted entires\n",
    "tempDF2.drop(tempDF2.index[56221:len(tempDF2.index)],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data spread- Cleanup tweet and username fields using preprocessor api library\n",
    "starttime = timeit.default_timer()\n",
    "print(\"The start time is :\",starttime)\n",
    "tempDF2['tweet']=''\n",
    "tempDF2['username']=''\n",
    "for i in tempDF2.index:\n",
    "    tempDF2['tweet'][i]= p.clean(tempDF2['text'].iloc[i])\n",
    "    tempDF2['username'][i]= p.clean(tempDF2['user'].iloc[i])\n",
    "    i += 1\n",
    "    print(\"i\", i)\n",
    "print(\"items updated already!\")\n",
    "print(\"The time difference is :\", timeit.default_timer() - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweetsDF.append(retweetsDF,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF1.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new dataframe\n",
    "NewtweetsDF= pd.concat([tempDF1,tempDF2],ignore_index = True)\n",
    "NewtweetsDF.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewtweetsDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capture only platform info\n",
    "NewtweetsDF['platform'] = NewtweetsDF['source'].str.extract('(>.*(?=</))')\n",
    "NewtweetsDF['platform'] = NewtweetsDF['platform'].str.replace('>','')\n",
    "NewtweetsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all but unnecessary columns\n",
    "# pandas drop columns using list of column names\n",
    "try: \n",
    "    delColNames = ['id_str','source','retweet']\n",
    "    NewtweetsDF.drop(delColNames,axis=1,inplace=True)\n",
    "    print(\"items deleted!\")\n",
    "except:\n",
    "    print(\"items deleted already!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NewtweetsDF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re arrange columns\n",
    "column_titles = ['id','datets'\n",
    "                 ,'date','year','month','monthday','hour','minute','secs','monthname','dayname'\n",
    "                 ,'user','username','followers','platform','text','tweet','coords']\n",
    "\n",
    "NewtweetsDF = NewtweetsDF.reindex(columns = column_titles)\n",
    "NewtweetsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total tweets\n",
    "print ('Total tweets this period:', len(NewtweetsDF.index), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "NewtweetsDF.to_csv(r'NewtweetsDF_08222020.csv', index = False, header=True)\n",
    "print('data exported successfully:')\n",
    "\n",
    "pd.read_csv('NewtweetsDF_08222020.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame containing text \"las\" from tweetsDF\n",
    "la_tweets = NewtweetsDF[NewtweetsDF['text'].str.contains('los angeles|latraffic|california|LAFD|LAPD')]\n",
    "la_tweets.shape\n",
    "# la_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find users having most followers\n",
    "\n",
    "user_followers = la_tweets[['user','followers']]\n",
    "#unique UserName\n",
    "user_followers = user_followers.drop_duplicates().sort_values('followers',ascending=False)\n",
    "user_followers = user_followers.set_index('user')\n",
    "print('Top 10 users by followers:')\n",
    "user_followers.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust display of decimals with comma separators on thousands\n",
    "# pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find users having most followers\n",
    "user_followers.reset_index(level=0,inplace = True,drop=False)\n",
    "user_followers.index += 1\n",
    "print('Top 10 users by followers:')\n",
    "user_followers_top10 = user_followers.head(10)\n",
    "user_followers_top10.drop_duplicates()\n",
    "#print('Top 10 users by followers: \\n')\n",
    "for index, row in user_followers_top10.iterrows():\n",
    "    print(row['user'],': ', '{:,.0f}'.format(row[\"followers\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_top10_plot = user_followers_top10.set_index('user')\n",
    "x = followers_top10_plot.plot(kind='barh', figsize=(10, 5), color='#86bf91', zorder=2, width=0.5)\n",
    "\n",
    "x.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\"\n",
    "               , left=\"off\", right=\"off\", labelleft=\"on\",labelsize = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find min,max, average followers\n",
    "max_followers = user_followers['followers'].max()\n",
    "avg_followers = user_followers['followers'].mean()\n",
    "min_followers = user_followers['followers'].min()\n",
    "print('Maximum Number of followers: ',max_followers)\n",
    "print('Avergage Number of followers: ',round(avg_followers,0))\n",
    "print('Minimum Number of followers: ',min_followers)\n",
    "\n",
    "pd.set_option('display.float_format',lambda x: '%.2f' % x)\n",
    "user_followers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validate Number of retweets by values \n",
    "\n",
    "retweets = la_tweets[la_tweets['text'].str.startswith('RT')==True]\n",
    "print('Number of retweets: ',len(retweets))\n",
    "print('Percentage of retweets {}%'.format(round((len(retweets))/len(la_tweets['text'])*100,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all direct tweets\n",
    "direct_tweets = la_tweets[la_tweets['text'].str.startswith('RT')==False]\n",
    "print('Number of actual tweets: ',len(direct_tweets))\n",
    "print('Percentage of actual tweets {}%'.format(round((len(direct_tweets))/len(la_tweets['text'])*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of tweets by date\n",
    "by_date = direct_tweets.groupby(['date']).size().reset_index(name='counts')\n",
    "by_date = by_date.set_index('date')\n",
    "by_date.reset_index(level=0,inplace = True,drop=False)\n",
    "by_date.index += 1\n",
    "print('Tweets by calender date: ')\n",
    "by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar chart tweets by calender date\n",
    "by_date_plot = by_date.set_index('date')\n",
    "\n",
    "ax = by_date_plot.plot(kind='bar', figsize=(10, 5), color='#86bf91', zorder=2, width=0.5)\n",
    "ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\"\n",
    "               , left=\"off\", right=\"off\", labelleft=\"on\",labelsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by_date.info()\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(by_date.date, by_date.counts, linewidth=.8)\n",
    "plt.plot(by_date.date, by_date.counts, '*', markersize=15, color='green')\n",
    "plt.xticks(fontsize=12, fontweight='regular',rotation=90)\n",
    "plt.yticks(fontsize=12, fontweight='regular')\n",
    "plt.xlabel('Dates',fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Total Count',fontsize=20, fontweight='bold')\n",
    "plt.title('Counts per time',fontsize=20, fontweight='bold')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of tweets by day\n",
    "by_month_by_weekday = direct_tweets.groupby(['monthname','dayname']).size().reset_index(name='counts')\n",
    "by_month_by_weekday = by_month_by_weekday.set_index('monthname')\n",
    "by_month_by_weekday.reset_index(level=0,inplace = True,drop=False)\n",
    "by_month_by_weekday.index += 1\n",
    "print('Tweets by week day of the Month: ')\n",
    "by_month_by_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of tweets by hour of day\n",
    "by_day_hour = direct_tweets.groupby(['dayname','hour']).size().reset_index(name='counts')\n",
    "by_day_hour = by_day_hour.set_index('dayname')\n",
    "by_day_hour.reset_index(level=0,inplace = True,drop=False)\n",
    "by_day_hour.index += 1\n",
    "print('Tweets by hour of the day: ')\n",
    "by_day_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_day_hour['day_hour'] = by_day_hour['dayname'] + '-'+ by_day_hour['hour'].astype('str')\n",
    "sel_columns =['day_hour','counts']\n",
    "by_day_hour_df = pd.DataFrame(by_day_hour,columns = sel_columns)\n",
    "# tweetsDF = pd.DataFrame(tw_list,columns=ColNames)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(by_day_hour_df.day_hour, by_day_hour_df.counts, linewidth=.8, color ='orange')\n",
    "plt.plot(by_day_hour_df.day_hour, by_day_hour_df.counts, '*', markersize=10, color='blue')\n",
    "plt.xticks(fontsize=12, fontweight='regular',rotation=90)\n",
    "plt.yticks(fontsize=12, fontweight='regular')\n",
    "plt.xlabel('Hour of Day',fontsize=15, fontweight='bold')\n",
    "plt.ylabel('Tweets Count',fontsize=15, fontweight='bold')\n",
    "plt.title('Number of Tweets over time',fontsize=15, fontweight='bold')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of tweets by hour of day\n",
    "by_platform = direct_tweets.groupby(['platform']).size().reset_index(name='counts').sort_values('counts',ascending=False)\n",
    "by_platform = by_platform.set_index('platform')\n",
    "by_platform.reset_index(level=0,inplace = True,drop=False)\n",
    "by_platform.index += 1\n",
    "print('Top 5 Platform by users:')\n",
    "by_platform_top5 = by_platform.head(5)\n",
    "by_platform_top5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use('fivethirtyeight')\n",
    "by_platform_top5 = by_platform.sort_values('counts').tail(5)\n",
    "by_platform_top5 = by_platform_top5.set_index('platform')\n",
    "\n",
    "ax = by_platform_top5.plot(kind='barh', figsize=(15, 5), color='#86bf91', zorder=2, width=0.5)\n",
    "ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\"\n",
    "               , left=\"off\", right=\"off\", labelleft=\"on\",labelsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_platform_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_platform_top5 = by_platform_top5.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax_pie = by_platform_top5.plot(kind='pie', title ='Top 5 Platform by users',legend =False \\\n",
    "#                                  , autopct='%1.1f%%', explode=(0,0,0,0.2,0) \\\n",
    "#                                 ,shadow = True, startangle=0,fontsize=10, subplots =True)\n",
    "\n",
    "# Pie chart\n",
    "\n",
    "platform = by_platform_top5['platform']\n",
    "counts = by_platform_top5['counts']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.pie(counts, labels = platform,autopct='%1.2f%%',explode=(0,0,0,0.2,0))\n",
    "ax.set_title('Top 5 Platform by users')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Wordcloud\n",
    "\n",
    "tweets = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "# Theme and styles for visuals\n",
    "plt.rcParams['font.family'] = \"calibri\" # font\n",
    "# sns.set_context('talk') # visuals outputted for presentation style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# iterate through the csv file \n",
    "for val in la_tweets.tweet: \n",
    "    # typecaste each val to string \n",
    "    val = str(val) \n",
    "#     print('text from tweets: ',len(val),'\\n',val)\n",
    "    # split the value \n",
    "    tokens = val.split() \n",
    "    # Converts each token into lowercase \n",
    "    for i in range(len(tokens)): \n",
    "        tokens[i] = tokens[i].lower() \n",
    "\n",
    "    for words in tokens: \n",
    "        tweets = tweets + words + ' '\n",
    "        \n",
    "# print('\\nWordcloud bag of words from tweets: ',len(tokens),'\\n',val)\n",
    "\n",
    "\n",
    "#update stopwords list\n",
    "stopwords.update(['https', 'rt','hi','co','promo', 'code','thank','th','ht'])\n",
    "\n",
    "custom_mask = np.array(Image.open(\"butterfly.png\")) #buterfly shape wordcloud\n",
    "\n",
    "#define wordcloud parameters\n",
    "# wordcloud = WordCloud(width = 1500, height = 900, max_words=70,\n",
    "#             background_color ='white',mask=custom_mask,\n",
    "#             stopwords = stopwords, \n",
    "#             min_font_size = 12).generate(tweets) \n",
    "\n",
    "wordcloud = WordCloud(max_words=50,width = 1500, height = 900,\n",
    "            background_color ='white',mask=custom_mask,\n",
    "            stopwords = stopwords).generate(tweets) \n",
    "\n",
    "# plot the WordCloud image                        \n",
    "plt.figure(figsize = (15, 10), facecolor = None) \n",
    "plt.imshow(wordcloud, interpolation='bilinear') \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list db names\n",
    "twdb.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop db\n",
    "# twdb.tweets.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tweet-preprocessor\n",
    "\n",
    "import preprocessor as p\n",
    "p.clean('RT @IslamRizza: #RP @slausongirl\\n• • • • •\\nBlack men held space in front of the LAPD he \\\n",
    "        adquarters in DTLA in support of @DocMellyMel 🙏🏾 It…')\n",
    "\n",
    "p.clean('Hurricane Awareness: Zephyr Insurance \\n\\nREAD MORE: https://t.co/SEhxK4bwyi\\n\\n#Accidents \\\n",
    "                #Claims #DisasterMitigation #Insurance #InsuranceTechnology #InsurTech #Points #RiskMitigation #Technology~ https://t.co/kCrl2YxHfK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
